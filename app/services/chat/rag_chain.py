"""
RAG Chain for Recruiter Assistant ChatBot.

Orchestrates the full RAG pipeline:
1. Get chat history from memory
2. Transform query (extract search query + filters)
3. Search candidates if needed
4. Build context and generate response with streaming
"""

import json
import logging
from typing import Optional, List, AsyncGenerator, Dict, Any

from groq import Groq

from app.config import get_settings
from app.schemas.chat import (
    ChatMessage,
    MessageRole,
    CandidateCard,
    TransformedQuery,
    RetrievedChunk,
)
from app.schemas.search import SearchRequest, SearchType
from app.services.chat.memory import ConversationMemory, get_conversation_memory
from app.services.chat.query_transformer import QueryTransformer, get_query_transformer
from app.services.search.hybrid import HybridSearchEngine, get_search_engine

settings = get_settings()
logger = logging.getLogger(__name__)


SYSTEM_PROMPT = """# ROLE
B·∫°n l√† tr·ª£ l√Ω tuy·ªÉn d·ª•ng th√¥ng minh. B·∫°n CH·ªà ƒë∆∞·ª£c ph√©p tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn ·ª©ng vi√™n trong h·ªá th·ªëng.

# QUY T·∫ÆC B·∫ÆT BU·ªòC (CRITICAL)

## 1. CH·ªà TR·∫¢ L·ªúI D·ª∞A TR√äN DATABASE
- B·∫°n CH·ªà ƒë∆∞·ª£c ph√©p tr·∫£ l·ªùi d·ª±a HO√ÄN TO√ÄN v√†o d·ªØ li·ªáu JSON trong ph·∫ßn [CONTEXT]
- N·∫øu KH√îNG C√ì d·ªØ li·ªáu trong [CONTEXT], b·∫°n PH·∫¢I n√≥i: "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p trong h·ªá th·ªëng."
- TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c b·ªãa ƒë·∫∑t, ph·ªèng ƒëo√°n, ho·∫∑c th√™m th√¥ng tin kh√¥ng c√≥ trong database

## 2. TRUNG TH·ª∞C V√Ä CH√çNH X√ÅC
- N·∫øu m·ªôt tr∆∞·ªùng l√† "null" ho·∫∑c kh√¥ng c√≥: N√≥i r√µ "Th√¥ng tin n√†y kh√¥ng c√≥ trong h·ªì s∆°"
- N·∫øu kh√¥ng t√¨m th·∫•y ·ª©ng vi√™n: N√≥i r√µ "Kh√¥ng t√¨m th·∫•y ·ª©ng vi√™n n√†o ph√π h·ª£p v·ªõi y√™u c·∫ßu"
- N·∫øu c√¢u h·ªèi n·∫±m ngo√†i ph·∫°m vi tuy·ªÉn d·ª•ng/CV: T·ª´ ch·ªëi l·ªãch s·ª±

## 3. PH·∫†M VI ƒê∆Ø·ª¢C PH√âP TR·∫¢ L·ªúI
‚úÖ ƒê∆∞·ª£c ph√©p:
- Th√¥ng tin ·ª©ng vi√™n (t√™n, email, s·ªë ƒëi·ªán tho·∫°i, k·ªπ nƒÉng, kinh nghi·ªám)
- T√¨m ki·∫øm ·ª©ng vi√™n theo ti√™u ch√≠ (skills, location, experience)
- So s√°nh ·ª©ng vi√™n d·ª±a tr√™n d·ªØ li·ªáu th·ª±c
- ƒê·∫øm s·ªë l∆∞·ª£ng ·ª©ng vi√™n trong database
- Th√¥ng tin v·ªÅ projects, education, certifications c·ªßa ·ª©ng vi√™n

‚ùå KH√îNG ƒë∆∞·ª£c ph√©p:
- Tr·∫£ l·ªùi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn tuy·ªÉn d·ª•ng/CV
- ƒê∆∞a ra nh·∫≠n x√©t ch·ªß quan kh√¥ng d·ª±a tr√™n d·ªØ li·ªáu
- Ph·ªèng ƒëo√°n v·ªÅ kh·∫£ nƒÉng, t√≠nh c√°ch c·ªßa ·ª©ng vi√™n
- Tr·∫£ l·ªùi v·ªÅ c√°c ch·ªß ƒë·ªÅ: ch√≠nh tr·ªã, t√¥n gi√°o, gi·∫£i tr√≠, tin t·ª©c...

## 4. ƒê·ªäNH D·∫†NG MARKDOWN
- D√πng `###` cho t√™n ·ª©ng vi√™n (e.g., `### üë§ VU VAN THANH`)
- D√πng b·∫£ng Markdown cho so s√°nh ho·∫∑c th√¥ng tin c√≥ c·∫•u tr√∫c
- D√πng danh s√°ch `-` cho skills
- PH·∫¢I c√≥ d√≤ng tr·ªëng tr∆∞·ªõc v√† sau heading, table, list

# V√ç D·ª§ PH·∫¢N H·ªíI ƒê√öNG

**Khi so s√°nh ·ª©ng vi√™n:**
```
D∆∞·ªõi ƒë√¢y l√† b·∫£ng so s√°nh gi·ªØa Nguyen Van A v√† Tran Van B:

| Ti√™u ch√≠ | üë§ NGUYEN VAN A | üë§ TRAN VAN B |
| :--- | :--- | :--- |
| **Kinh nghi·ªám** | 5 nƒÉm (Senior) | 3 nƒÉm (Mid-level) |
| **K·ªπ nƒÉng ch√≠nh** | Python, DevOps, AWS | Python, Django, React |
| **ƒêi·ªÉm m·∫°nh** | C√≥ ch·ª©ng ch·ªâ AWS, kinh nghi·ªám d·ªìi d√†o | Fullstack, ti·∫øng Anh t·ªët |

**K·∫øt lu·∫≠n:**
- N·∫øu c·∫ßn v·ªã tr√≠ thi√™n v·ªÅ h·∫° t·∫ßng/backend s√¢u: Ch·ªçn **Nguyen Van A**.
- N·∫øu c·∫ßn l√†m s·∫£n ph·∫©m nhanh (Fullstack): Ch·ªçn **Tran Van B**.
```

**Khi t√¨m th·∫•y ·ª©ng vi√™n:**
```
### üë§ NGUYEN VAN A

| Th√¥ng tin | Chi ti·∫øt |
| :--- | :--- |
| **Email** | example@email.com |
| **Kinh nghi·ªám** | 5.2 nƒÉm |

**K·ªπ nƒÉng:** Python, FastAPI, Docker
```

**Khi KH√îNG t√¨m th·∫•y:**
"Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y ·ª©ng vi√™n n√†o ph√π h·ª£p v·ªõi ti√™u ch√≠ 'Data Scientist t·∫°i ƒê√† N·∫µng' trong h·ªá th·ªëng. Hi·ªán t·∫°i database c√≥ 2 ·ª©ng vi√™n."

**Khi c√¢u h·ªèi ngo√†i ph·∫°m vi:**
"Xin l·ªói, t√¥i ch·ªâ c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn tuy·ªÉn d·ª•ng v√† th√¥ng tin ·ª©ng vi√™n trong h·ªá th·ªëng. B·∫°n c√≥ th·ªÉ h·ªèi t√¥i: 'T√¨m ·ª©ng vi√™n Python', 'Ai c√≥ kinh nghi·ªám React?', 'Th√¥ng tin chi ti·∫øt v·ªÅ ·ª©ng vi√™n X'."

# EXECUTION
Lu√¥n ƒë·ªçc k·ªπ [CONTEXT], ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu th·ª±c, v√† t·ª´ ch·ªëi l·ªãch s·ª± n·∫øu kh√¥ng c√≥ th√¥ng tin ho·∫∑c c√¢u h·ªèi ngo√†i ph·∫°m vi."""

CONTEXT_TEMPLATE = """
[CONTEXT] 
{candidate_context}
"""

PARSING_RECOVERY_PROMPT = """
# PARSING ERROR RECOVERY MODE

‚ö†Ô∏è The user has indicated that the requested data DOES exist in the profile, but the previous extraction failed. You must perform a deep re-read of the raw text.

## STRICT INSTRUCTIONS:
1. **DO NOT use default answers** like "Not mentioned", "Kh√¥ng r√µ", "N/A" etc.
2. **Re-read the ENTIRE raw text** of each candidate carefully, line by line.
3. If you cannot find the candidate's name, use "·ª®ng vi√™n [S·ªë th·ª© t·ª±]" (e.g., "·ª®ng vi√™n 1", "·ª®ng vi√™n 2").
4. **Search for keywords** in the raw text such as: Python, Kinh nghi·ªám, N∆°i l√†m vi·ªác, H·ªçc v·∫•n, K·ªπ nƒÉng, etc.
5. **Extract content greedily**: Even if the format doesn't match a perfect table structure, extract any text near the relevant keywords.
6. If you find partial information (e.g., "3 nƒÉm l√†m vi·ªác t·∫°i c√¥ng ty ABC"), format it as best you can.

## EXAMPLE OUTPUT FOR INCOMPLETE DATA:
```
### üë§ ·ª®ng vi√™n 1

| Field | Description |
| :--- | :--- |
| **T√™n** | (Tr√≠ch t·ª´ vƒÉn b·∫£n: "Nguy·ªÖn VƒÉn A") |
| **Kinh nghi·ªám** | ~3 nƒÉm (Tr√≠ch: "ƒë√£ l√†m vi·ªác 3 nƒÉm t·∫°i C√¥ng ty XYZ") |
| **K·ªπ nƒÉng Python** | ‚úÖ C√≥ ƒë·ªÅ c·∫≠p (Tr√≠ch: "s·ª≠ d·ª•ng Python trong d·ª± √°n AI") |

**Ghi ch√∫:** H·ªì s∆° n√†y c√≥ ƒë·ªãnh d·∫°ng kh√¥ng chu·∫©n, th√¥ng tin tr√™n ƒë∆∞·ª£c tr√≠ch xu·∫•t th·ªß c√¥ng t·ª´ vƒÉn b·∫£n th√¥.
```

Now, re-process the [CONTEXT] data using these recovery rules."""



class RAGChain:
    """
    Main RAG orchestration chain for the ChatBot.
    
    Combines:
    - Conversation memory (Redis)
    - Query transformation (LLM)
    - Hybrid search (BM25 + Vector)
    - Response generation with streaming
    """
    
    def __init__(
        self,
        memory: Optional[ConversationMemory] = None,
        transformer: Optional[QueryTransformer] = None,
        search_engine: Optional[HybridSearchEngine] = None,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
    ):
        """Initialize RAG chain with dependencies."""
        self.memory = memory or get_conversation_memory()
        self.transformer = transformer or get_query_transformer()
        self.search_engine = search_engine or get_search_engine()
        self.api_key = api_key or settings.groq_api_key
        self.model = model or settings.chat_model
        self._client: Optional[Groq] = None
        # Track retrieved chunks per session for debug output
        self._last_retrieved_chunks: Dict[str, List[RetrievedChunk]] = {}

    
    def _get_client(self) -> Groq:
        """Get or create Groq client."""
        if self._client is None:
            self._client = Groq(api_key=self.api_key)
        return self._client
    
    async def chat(
        self,
        session_id: str,
        message: str,
        db_session,
    ) -> AsyncGenerator[tuple[str, str], None]:
        """
        Process a chat message and stream the response.
        
        Args:
            session_id: Unique session identifier
            message: User message
            db_session: Database session for search
            
        Yields:
            (type, content) tuples where type is 'token' or 'status'
        """
        # Step 1: Get conversation history
        history = await self.memory.get_history(session_id)
        logger.info(f"Session {session_id}: Got {len(history)} messages from history")
        
        # Step 2: Save user message to history
        await self.memory.add_message(
            session_id=session_id,
            role=MessageRole.USER,
            content=message,
        )
        
        # Step 3: Transform query
        yield ("status", "ƒêang ph√¢n t√≠ch y√™u c·∫ßu...")
        transformed = await self.transformer.transform_async(message, history)
        logger.info(f"Transformed query: {transformed.search_query}, intent: {transformed.intent}")
        
        # Step 3.5: Handle off-topic questions
        if transformed.intent == "off_topic":
            off_topic_response = (
                "Xin l·ªói, t√¥i ch·ªâ c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn **tuy·ªÉn d·ª•ng v√† th√¥ng tin ·ª©ng vi√™n** trong h·ªá th·ªëng.\n\n"
                "B·∫°n c√≥ th·ªÉ h·ªèi t√¥i nh·ªØng c√¢u nh∆∞:\n"
                "- üîç \"T√¨m ·ª©ng vi√™n Python Developer\"\n"
                "- üìã \"Ai c√≥ kinh nghi·ªám React?\"\n"
                "- üë§ \"Th√¥ng tin chi ti·∫øt v·ªÅ ·ª©ng vi√™n Vu Van Thanh\"\n"
                "- üìä \"So s√°nh 2 ·ª©ng vi√™n c√≥ k·ªπ nƒÉng Machine Learning\"\n"
                "- üè¢ \"C√≥ ·ª©ng vi√™n n√†o ·ªü H√† N·ªôi kh√¥ng?\""
            )
            yield ("token", off_topic_response)
            
            # Save to history
            await self.memory.add_message(
                session_id=session_id,
                role=MessageRole.ASSISTANT,
                content=off_topic_response,
            )
            return
        
        # Step 3.6: Handle list_all intent - fetch all candidates from DB
        candidates: List[CandidateCard] = []
        candidate_context = ""
        
        if transformed.intent == "list_all":
            yield ("status", "ƒêang t·∫£i danh s√°ch ·ª©ng vi√™n...")
            
            from sqlalchemy import select
            from app.models.candidate import Candidate
            
            # Fetch all candidates
            result = await db_session.execute(
                select(Candidate).order_by(Candidate.created_at.desc()).limit(20)
            )
            all_candidates = result.scalars().all()
            
            if all_candidates:
                # Build candidate cards
                candidates = []
                json_context_data = []
                
                for cand in all_candidates:
                    card = CandidateCard(
                        candidate_id=cand.id,
                        full_name=cand.full_name,
                        headline=cand.headline,
                        total_experience_years=cand.total_experience_years or 0,
                        top_skills=cand.top_skills[:5] if cand.top_skills else [],
                        email=cand.email,
                        # phone=cand.phone,  # Schema does not have phone
                    )
                    candidates.append(card)
                    
                    # Build context for LLM
                    json_context_data.append({
                        "name": cand.full_name,
                        "email": cand.email,
                        "phone": cand.phone,
                        "headline": cand.headline,
                        "experience_years": cand.total_experience_years,
                        "skills": cand.top_skills[:10] if cand.top_skills else [],
                        "summary": cand.summary,
                    })
                
                import json
                candidate_context = CONTEXT_TEMPLATE.format(
                    candidate_context=json.dumps(json_context_data, ensure_ascii=False, indent=2)
                )
                candidate_context = f"[DATABASE INFO] T·ªïng s·ªë ·ª©ng vi√™n: {len(all_candidates)}. D∆∞·ªõi ƒë√¢y l√† danh s√°ch:\n\n" + candidate_context
                
                logger.info(f"List all: Found {len(all_candidates)} candidates")
            else:
                candidate_context = "[DATABASE INFO] Hi·ªán t·∫°i ch∆∞a c√≥ ·ª©ng vi√™n n√†o trong h·ªá th·ªëng."
        
        # Step 4: Search candidates if needed (for specific search queries)
        elif transformed.is_search_needed and (transformed.semantic_query or transformed.keyword_string):
            yield ("status", "ƒêang t√¨m ki·∫øm v√† ƒë√°nh gi√° h·ªì s∆°...")
            # Build search request with filters
            search_request = SearchRequest(
                query=transformed.semantic_query, # Use semantic query for vector
                keyword_query=transformed.keyword_string, # Use keyword string for BM25
                search_type=SearchType.HYBRID,
                expand_query=True,
                top_k=settings.chat_max_candidates,
                min_experience_years=transformed.filters.get("min_experience_years"),
                required_skills=transformed.filters.get("required_skills", []),
                location=transformed.filters.get("location"),
            )
            
            try:
                # Execute 3-Layer Search Strategy
                candidates, retrieved_chunks, search_note = await self._search_with_fallback(
                    search_request=search_request,
                    db_session=db_session
                )
                
                # Build JSON context
                json_context_data = []
                for i, card in enumerate(candidates[:settings.chat_max_candidates], 1):
                    # Find chunks for this candidate
                    matches = [c for c in retrieved_chunks if c.candidate_name == card.full_name]
                    
                    candidate_data = {
                        "full_name": card.full_name,
                        "headline": card.headline,
                        "total_experience_years": card.total_experience_years,
                        "top_skills": card.top_skills,
                        "email": card.email,
                        "match_score": card.match_score,
                        "retrieved_sections": [
                            {
                                "section": m.section,
                                "content": m.content,
                                "score": m.score
                            }
                            for m in matches
                        ]
                    }
                    json_context_data.append(candidate_data)

                self._last_retrieved_chunks[session_id] = retrieved_chunks
                
                if json_context_data:
                    import json
                    candidate_context_json = json.dumps(json_context_data, ensure_ascii=False, indent=2)
                    
                    candidate_context = CONTEXT_TEMPLATE.format(
                        candidate_context=candidate_context_json
                    )
                    # Add search note to context (if relaxed search was used)
                    if search_note:
                         candidate_context = f"L∆ØU √ù H·ªÜ TH·ªêNG: {search_note}\n\n" + candidate_context
                else:
                    # Get total count for context
                    from sqlalchemy import func, select
                    from app.models.candidate import Candidate
                    count_result = await db_session.execute(select(func.count(Candidate.id)))
                    total_count = count_result.scalar() or 0
                    candidate_context = f"[DATABASE INFO] Kh√¥ng t√¨m th·∫•y ·ª©ng vi√™n ph√π h·ª£p v·ªõi ti√™u ch√≠. T·ªïng s·ªë ·ª©ng vi√™n trong h·ªá th·ªëng: {total_count}."
                    
                logger.info(f"Found {len(candidates)} candidates via strategy.")

                
            except Exception as e:
                logger.error(f"Search failed: {e}")
                candidate_context = f"L·ªói khi t√¨m ki·∫øm: {str(e)}"
        
        # Step 5: Build messages for LLM
        # Check if user is requesting parsing recovery (frustrated with "not mentioned" answers)
        recovery_keywords = [
            "l·ªói", "parsing error", "b·ªã l·ªói", "kh√¥ng ƒë√∫ng", "ƒë√£ c√≥ trong h·ªì s∆°",
            "d·ªØ li·ªáu c√≥ t·ªìn t·∫°i", "th·ª≠ l·∫°i", "retry", "tr√≠ch xu·∫•t l·∫°i",
            "kh√¥ng t√¨m th·∫•y", "sai r·ªìi", "l∆∞·ªùi"
        ]
        is_recovery_request = any(kw in message.lower() for kw in recovery_keywords)
        
        if is_recovery_request:
            logger.info(f"Detected parsing recovery request from user")
            messages = [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "system", "content": PARSING_RECOVERY_PROMPT},
            ]
        else:
            messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        
        # Add context if we have candidates
        if candidate_context:
            messages.append({"role": "system", "content": candidate_context})
        
        # Add conversation history (last few messages)
        for msg in history[-6:]:
            messages.append({
                "role": msg.role.value,
                "content": msg.content,
            })
        
        # Add current user message
        messages.append({"role": "user", "content": message})
        
        # Step 6: Generate response with quality evaluation loop
        yield ("status", "ƒêang t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi...")
        client = self._get_client()
        
        # Import critic for response evaluation
        from app.services.chat.response_critic import get_response_critic
        critic = get_response_critic()
        
        full_response = ""
        best_response = ""
        best_score = 0.0
        max_attempts = 3
        
        for attempt in range(max_attempts):
            current_response = ""
            
            try:
                # Build messages for this attempt
                attempt_messages = messages.copy()
                
                # If retry, add critic feedback
                if attempt > 0 and hasattr(self, '_last_critic_result'):
                    feedback_prompt = critic.get_regeneration_prompt(self._last_critic_result)
                    attempt_messages.insert(1, {"role": "system", "content": feedback_prompt})
                    yield ("status", f"ƒêang c·∫£i thi·ªán c√¢u tr·∫£ l·ªùi (l·∫ßn {attempt + 1})...")
                
                stream = client.chat.completions.create(
                    model=self.model,
                    messages=attempt_messages,
                    temperature=0.7 if attempt == 0 else 0.5,  # Lower temp on retry
                    max_tokens=1000,
                    stream=True,
                )
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        token = chunk.choices[0].delta.content
                        current_response += token
                        # Buffer response, do not stream yet
                
                # Evaluate response quality
                if attempt < max_attempts - 1:
                    yield ("status", "ƒêang ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi...")
                    critic_result = critic.evaluate(message, current_response, candidate_context)
                    
                    logger.info(f"Response critic score (attempt {attempt + 1}): {critic_result.score:.1f}/10")
                    
                    # Keep track of best response
                    if critic_result.score > best_score:
                        best_score = critic_result.score
                        best_response = current_response
                    
                    # Check if good enough
                    if not critic.should_retry(critic_result, attempt):
                        full_response = current_response
                        break
                    
                    # Store for next iteration
                    self._last_critic_result = critic_result
                else:
                    # Final attempt - use this response
                    full_response = current_response
                    
            except Exception as e:
                logger.error(f"LLM generation failed (attempt {attempt + 1}): {e}")
                if attempt == max_attempts - 1:
                    error_msg = f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}"
                    full_response = error_msg
                continue
        
        # Use best response if current is worse
        if best_score > 0 and not full_response:
            full_response = best_response
        
        # Yield the final response immediately for maximum speed
        # User requested to remove streaming effect for better performance
        chunk_size = 1024  # Large chunk size
        for i in range(0, len(full_response), chunk_size):
            chunk = full_response[i:i + chunk_size]
            yield ("token", chunk)
            # No sleep -> Instant return
        
        # Step 7: Save assistant response to history
        await self.memory.add_message(
            session_id=session_id,
            role=MessageRole.ASSISTANT,
            content=full_response,
            candidates=candidates,
        )
        
        logger.info(f"Session {session_id}: Completed response ({len(full_response)} chars)")
    
    async def get_candidates_from_last_response(
        self,
        session_id: str,
    ) -> List[CandidateCard]:
        """Get candidate cards from the last assistant message."""
        history = await self.memory.get_history(session_id, limit=1)
        
        for msg in reversed(history):
            if msg.role == MessageRole.ASSISTANT and msg.candidates:
                return msg.candidates
        
        return []

    async def _search_with_fallback(
        self,
        search_request: SearchRequest,
        db_session
    ) -> tuple[List[CandidateCard], List[RetrievedChunk], str]:
        """
        Execute search with 3-Layer Fallback Strategy.
        
        Returns:
            (candidates, debug_chunks, search_note)
        """
        candidates = []
        retrieved_chunks = []
        search_note = ""

        # === LAYER 1: STRICT SEARCH ===
        logger.info("Layer 1: Strict Search")
        response = await self.search_engine.search(search_request, db_session)
        
        if response.results:
            logger.info(f"Layer 1 success: Found {len(response.results)} candidates")
            return self._process_search_results(response) + ("",)

        # === LAYER 2: RELAXED SEARCH (Soft Filters) ===
        # Create relaxed request: Remove location, reduce experience
        logger.info("Layer 1 empty. Trying Layer 2: Relaxed Search")
        
        relaxed_request = SearchRequest(
            query=search_request.query,
            search_type=SearchType.HYBRID,
            expand_query=True,
            top_k=search_request.top_k,
            # Remove location filter
            location=None, 
            # Reduce experience requirement by 30% if exists, or remove if < 1 year
            min_experience_years=max(0.0, search_request.min_experience_years * 0.7) if search_request.min_experience_years else None,
            required_skills=search_request.required_skills, # Keep skills strictly for now
        )
        
        response = await self.search_engine.search(relaxed_request, db_session)
        if response.results:
            search_note = "Kh√¥ng t√¨m th·∫•y ·ª©ng vi√™n kh·ªõp 100% ti√™u ch√≠ (Layer 1). ƒê√¢y l√† c√°c ·ª©ng vi√™n G·∫¶N ƒê√öNG NH·∫§T (ƒë√£ n·ªõi l·ªèng ti√™u ch√≠ Location/Experience)."
            logger.info(f"Layer 2 success: Found {len(response.results)} candidates")
            return self._process_search_results(response) + (search_note,)

        # === LAYER 3: SEMANTIC FALLBACK (Vector Only) ===
        # Remove all filters, just semantic search
        logger.info("Layer 2 empty. Trying Layer 3: Semantic Fallback")
        
        fallback_request = SearchRequest(
            query=search_request.query,
            search_type=SearchType.SEMANTIC, # Pure vector search
            expand_query=True,
            top_k=3, # Limit to top 3 for fallback
            location=None,
            min_experience_years=None,
            required_skills=[], # Remove skill filter too
        )
        
        response = await self.search_engine.search(fallback_request, db_session)
        if response.results:
            search_note = "Kh√¥ng t√¨m th·∫•y ·ª©ng vi√™n ph√π h·ª£p ti√™u ch√≠ l·ªçc. ƒê√¢y l√† nh·ªØng ·ª©ng vi√™n c√≥ n·ªôi dung h·ªì s∆° LI√äN QUAN NH·∫§T theo √Ω nghƒ©a (Semantic Match)."
            logger.info(f"Layer 3 success: Found {len(response.results)} candidates")
            return self._process_search_results(response) + (search_note,)

        return [], [], "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu n√†o, k·ªÉ c·∫£ khi tra c·ª©u ng·ªØ nghƒ©a."

    def _process_search_results(self, response) -> tuple[List[CandidateCard], List[RetrievedChunk]]:
        """Convert SearchResponse to CandidateCards and RetrievedChunks."""
        candidates = []
        retrieved_chunks = []
        
        for result in response.results:
            # Create candidate card
            card = CandidateCard(
                candidate_id=result.candidate_id,
                full_name=result.full_name or "Unknown",
                headline=result.headline,
                email=result.email,
                total_experience_years=result.total_experience_years,
                top_skills=result.top_skills[:5] if result.top_skills else [],
                match_score=result.combined_score,
            )
            candidates.append(card)
            
            # Store chunks
            for chunk in result.matched_chunks[:3]:
                retrieved_chunks.append(
                    RetrievedChunk(
                        chunk_id=chunk.chunk_id,
                        candidate_name=card.full_name,
                        section=chunk.section,
                        content=chunk.content[:500],
                        score=chunk.score,
                        match_type=chunk.match_type,
                    )
                )
        return candidates, retrieved_chunks

    def get_retrieved_chunks(self, session_id: str) -> List[RetrievedChunk]:
        """
        Get the retrieved chunks from the last search for debugging.
        
        Returns:
            List of RetrievedChunk with content from CV sections used in response.
        """
        return self._last_retrieved_chunks.get(session_id, [])



# Singleton instance
_chain: Optional[RAGChain] = None


def get_rag_chain() -> RAGChain:
    """Get or create the RAG chain singleton."""
    global _chain
    if _chain is None:
        _chain = RAGChain()
    return _chain
